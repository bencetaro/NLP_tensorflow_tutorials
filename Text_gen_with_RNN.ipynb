{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text generator notebook from TF"
      ],
      "metadata": {
        "id": "OK613HGwDSxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare's writing from Andrej Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks. Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly."
      ],
      "metadata": {
        "id": "J0KseiT1DfKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "- The model is character based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "- The structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "- As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
      ],
      "metadata": {
        "id": "iiXBDmrxFwJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "kMwwVJEzDqTg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "metadata": {
        "id": "stK52_PuDqK2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the data"
      ],
      "metadata": {
        "id": "7pLeSaZbHebS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of tect is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8hRERVzG6HN",
        "outputId": "5ef579bb-7f7d-490a-c3e6-aadff4975523"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAu8gY6cG58Q",
        "outputId": "b0135295-a1bc-4f25-8611-8e663fbfe78b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text)) # ...cuz sets can allow only unique elements\n",
        "print(f'{len(vocab)} unique characters')\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PPBzHz3G51Q",
        "outputId": "a5d1af18-4584-4efa-a759-d2c020a59a19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the text"
      ],
      "metadata": {
        "id": "d-vaU83aIwj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tf.keras.layers.StringLookup layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ],
      "metadata": {
        "id": "_TcZIbenKqDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIepiAkmG5uu",
        "outputId": "15b91cd5-1ee7-4438-8a6f-26d44132b09e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now with the tf.keras.layers.StringLookup layer (vectorizer):\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "kpQXTylCDqCm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7stAmjXKuoQ",
        "outputId": "164f4b5b-fb4a-4a6c-9e38-f61d59810dc7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use tf.keras.layers.StringLookup(..., invert=True)."
      ],
      "metadata": {
        "id": "RNAXjADiLSNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# So this layer will be a decoder by only changing invert=True  \n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "H4e_Gk2lKbU8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check how the decoding went:\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OBGi3B4KbIH",
        "outputId": "4664da52-3b90-494b-82cb-d3e10319a4bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What if i pass in some random ints?\n",
        "myints = tf.constant([31,32,46,60,12])\n",
        "\n",
        "mychars = chars_from_ids(myints)\n",
        "mychars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl-gg8aaMMvp",
        "outputId": "2f14038b-3168-4958-e9d0-67c9d13bd814"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'R', b'S', b'g', b'u', b';'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can tf.strings.reduce_join to join the characters back into strings."
      ],
      "metadata": {
        "id": "vcZlX5TlNXm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e8h6mcFKa_q",
        "outputId": "994f76ee-29db-4253-93e5-103ba51b3255"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "19KxOKnkKa4G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training examples and targets"
      ],
      "metadata": {
        "id": "RTjdxqzbPPTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in all the text into tokenized form\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIhHkBhINmE3",
        "outputId": "06bfedc8-e4fd-4cce-e056-11981c70dbe3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset \n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "VEaj4ui2Nl4Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkout the decoded characters of first 10 sample token\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSzCYY3ENls5",
        "outputId": "9b589996-107e-4abf-da40-e1e1730e8726"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch method lets you easily convert these individual characters to sequences of the desired size."
      ],
      "metadata": {
        "id": "ilN9cqU5Sucj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qof1W3-NlkF",
        "outputId": "715e9760-609d-43f2-b5ce-1995fc373420"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings (using the previously defined text_from_ids - which joins the chars together):"
      ],
      "metadata": {
        "id": "uM7HCj0uS2XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H34GZ5dOKawB",
        "outputId": "5c22c533-5baa-4c23-e7bc-64d5c9e6a3ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "3Fk1RtFnUTrx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_yeYRdnA5vt",
        "outputId": "a6caf19f-d74e-41ed-92a5-f0ad8757ac5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Sayonara Tako kun', 'ayonara Tako kun!')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1] # \n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "sample = split_input_target(\"Sayonara Tako kun!\")\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply it on the whole, batched dataset\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "U1ZFScIUDKy6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YasPcmTDKkE",
        "outputId": "bcf29f4a-3fd9-4d6f-d5e9-297ddc9f22ee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training batches"
      ],
      "metadata": {
        "id": "4hJOhPUuVgAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also a fastens things up if we use prefetch, and also recommended to use shuffle for efficient training:"
      ],
      "metadata": {
        "id": "9vKXjtlYVsAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RctDwyrODKdh",
        "outputId": "6a92fdc7-9b72-444e-b299-11247a90fa3c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the model"
      ],
      "metadata": {
        "id": "ynb0bgHPWS6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "# so each character will receive a 256 long embedding array of floats\n",
        "# 256*65 total number\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "Xgyz61hTDKPu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # To convert input into embeddings\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True) # gru model\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size) # simple classif layer at the end\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "QJM9dC9pDJ-S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "FQ5oB5Q-DJxw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens inside the model:\n",
        "\n",
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character."
      ],
      "metadata": {
        "id": "jkoqxjxomiX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets run the model to see the shape of the output\n",
        "# (currently untrained model...)\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "id": "-LZ_0JCpDJlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69f4886-274c-4bb0-920a-932ab5dda4cb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khNYLnN40ZRo",
        "outputId": "07b5995c-f137-4c56-f09c-4a0627a78a3b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions for the 100 length input\n",
        "example_batch_predictions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV2cjxlErnPv",
        "outputId": "9fc3e7c5-f852-4cfb-e9bc-abdd0dd29181"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100, 66), dtype=float32, numpy=\n",
              "array([[ 4.95469756e-03,  7.27180531e-03,  1.11259669e-02, ...,\n",
              "         3.77608952e-03,  4.57991520e-03,  9.24432278e-03],\n",
              "       [ 4.88151005e-03,  6.51077088e-03,  1.09781660e-02, ...,\n",
              "         1.41227548e-03,  1.90219488e-02, -4.17208765e-03],\n",
              "       [ 7.59055000e-03,  3.03673605e-03, -5.88342547e-04, ...,\n",
              "        -7.74133345e-03,  3.46392533e-03,  3.63860303e-03],\n",
              "       ...,\n",
              "       [ 7.24139344e-03,  4.52696532e-03, -4.19180491e-04, ...,\n",
              "        -9.41982027e-03,  8.46317969e-04,  3.12187197e-03],\n",
              "       [ 3.66267562e-03,  2.37391400e-03, -5.16713597e-03, ...,\n",
              "        -7.21296808e-03,  2.83708563e-03, -8.75522383e-03],\n",
              "       [ 1.05778305e-02, -3.76375811e-03, -3.40450415e-03, ...,\n",
              "        -7.07788998e-03, -9.20849852e-05,  8.58471380e-04]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_cls_pred = tf.argmax(example_batch_predictions[0], axis=1)\n",
        "example_batch_cls_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwBS2W0nrtL_",
        "outputId": "ac563716-c7ad-423f-d8e5-d2ab28e1348d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
              "array([40, 64, 11, 11, 43, 64, 25, 63, 29, 50, 37, 25, 27, 27, 27, 27,  5,\n",
              "       65, 43, 64, 50, 10, 47, 43, 44, 28,  4, 11, 11, 10, 50, 22, 27, 46,\n",
              "       50, 50, 50, 11, 34, 40,  0,  8, 34, 62, 56, 25, 34, 18, 63, 46, 50,\n",
              "        5, 34, 21, 40, 27, 27, 43, 50, 64, 50, 50, 10, 25, 15, 33, 28, 35,\n",
              "        7,  7, 16, 14, 11, 34, 41, 41, 45, 18, 43, 11, 11, 23, 40, 25, 40,\n",
              "       40, 44, 50, 50, 50, 50, 11, 50, 50, 50, 18, 50, 11, 50, 23])>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# But as the notebook states we should do it like this:\n",
        "# (Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.)\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTsc3GehtP-5",
        "outputId": "84e70611-a5ad-4f09-92eb-e8c3d809dd90"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10, 26, 52,  6,  6, 16, 15, 24, 19, 16, 21, 17,  5, 31,  3, 47,  2,\n",
              "       60, 18, 26, 12, 34, 41, 54, 25,  0, 44,  9, 56, 34, 17, 50, 41, 28,\n",
              "       30, 64, 23, 28, 29, 47, 26, 21, 32, 18, 32,  6,  4, 20,  4, 64, 39,\n",
              "       47, 57, 37, 57, 52, 51, 43, 30, 56, 36, 45, 58, 24, 43, 53, 49, 51,\n",
              "       58, 45, 61, 47, 16, 38, 39,  5, 31, 22, 45, 56, 52, 65, 57,  5, 14,\n",
              "       34, 42, 57, 32, 51, 48, 43, 49, 51, 24, 48, 43, 35, 54,  3])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ],
      "metadata": {
        "id": "cbus1hObto0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtnrTmszpz2J",
        "outputId": "5dff9913-bb4e-484a-d254-66fd0be71da0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' that dares love attempt;\\nTherefore thy kinsmen are no let to me.\\n\\nJULIET:\\nIf they do see thee, they'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"3Mm''CBKFCHD&R!h uEM;UboL[UNK]e.qUDkbOQyJOPhMHSES'$G$yZhrXrmldQqWfsKdnjlsfvhCYZ&RIfqmzr&AUcrSlidjlKidVo!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "n4J0ElQjtukA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ],
      "metadata": {
        "id": "BENEpmDbt5e-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attach an optimizer, and a loss function\n",
        "\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "(bcuz the targets are label encoded...)\n",
        "\n",
        "Because your model returns logits, you need to set the from_logits flag."
      ],
      "metadata": {
        "id": "I6sLhilJuE5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "sTcNh3J6tdDn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU1oXyhdpzr7",
        "outputId": "e1f360c9-f508-4387-ab0a-5b82cd86a1e4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189888, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ],
      "metadata": {
        "id": "ejX1Q1MrxbGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xjd97Skuon7",
        "outputId": "a37d1c72-0c92-4c0d-da01-3e62d8c80972"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.015396"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the training procedure using the tf.keras.Model.compile method. Use tf.keras.optimizers.Adam with default arguments and the loss function."
      ],
      "metadata": {
        "id": "ZpHE0KK-uoLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "GNrXU0LTun1T"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure checkpoints\n",
        "\n",
        "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:\n",
        "\n"
      ],
      "metadata": {
        "id": "bNWhkXHcx5Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "YycdzomiunpB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute the training\n",
        "\n",
        "To keep training time reasonable, use 10 epochs to train the model. In Colab set the runtime to GPU for faster training."
      ],
      "metadata": {
        "id": "TLMqwJpuyT0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=20"
      ],
      "metadata": {
        "id": "mtww-rospzkP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im1ThLx6pzc3",
        "outputId": "843acaa6-b573-4f33-a38a-47771fb3cd6e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 15s 58ms/step - loss: 2.7283 - accuracy: 0.2807\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.9961 - accuracy: 0.4161\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.7195 - accuracy: 0.4905\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.5557 - accuracy: 0.5335\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.4560 - accuracy: 0.5593\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.3875 - accuracy: 0.5761\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.3337 - accuracy: 0.5893\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 1.2897 - accuracy: 0.6007\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.2490 - accuracy: 0.6111\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2088 - accuracy: 0.6216\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1706 - accuracy: 0.6316\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.1293 - accuracy: 0.6438\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0877 - accuracy: 0.6559\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0419 - accuracy: 0.6695\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9944 - accuracy: 0.6841\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9444 - accuracy: 0.6995\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8920 - accuracy: 0.7169\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8401 - accuracy: 0.7331\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7891 - accuracy: 0.7500\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7384 - accuracy: 0.7666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latest = tf.train.latest_checkpoint('/content/training_checkpoints/')\n",
        "print(f'Latest model: {latest}')\n",
        "model_loaded = model.load_weights(latest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcswYv9cFZPx",
        "outputId": "b00f75fb-c563-42e6-ea7f-7dd930b6c5f0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest model: /content/training_checkpoints/ckpt_20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text\n",
        "\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it. Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n",
        "\n",
        "The following makes a single step prediciton."
      ],
      "metadata": {
        "id": "qtJCTmA-yxT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "m3bIHT3ByECB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate next char\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "C_W2_-XiyD6Z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ],
      "metadata": {
        "id": "DqLZtiia2naR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zALwP8jhyDxk",
        "outputId": "4bca069b-c46d-4bf8-86e3-a424dccbc946"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "O, hust thou fox us roat,\n",
            "Becours that offer me dear Jupitard,\n",
            "That you shall cure it, subdue the Roman,\n",
            "The prize of long past cure of this gentleman,\n",
            "That you are yours, sleeps in't, and their loves and\n",
            "remembrance will speak to their names. Johemia, some\n",
            "conscience to a consoot, you do awmile me.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Hast thou tail'd with the sleeping office?\n",
            "Here, sir; as well used the realm; but they have cook'd\n",
            "so fond to ambition. Their war,\n",
            "When he shall find it meet and bay sent,--\n",
            "Not arm'd on one that seestern such\n",
            "A foolmand in this business.\n",
            "\n",
            "POLIXENES:\n",
            "How!\n",
            "When good my life upon you! with my father, I do believe\n",
            "Her father to my throne? but to make an end o'er the\n",
            "eventness of a miserable bar, and blunts,\n",
            "And bid them babies, compellons dalkners to the good am\n",
            "I never talk; one that prover chaste,\n",
            "For my past made gobed steel away,\n",
            "Lord marshal, and Master not to field or strive,\n",
            "Whilvest thou sleep, there's never yet sufficiently,\n",
            "And givens every hand in falsehood of our  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.0612640380859375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try EPOCHS = 30).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.\n",
        "\n",
        "If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ],
      "metadata": {
        "id": "GzVuI3kw3OEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "for i in range(len(next_char)):\n",
        "    print(result[i].numpy().decode('utf-8') + '\\n' + '_'*80) # to print out the 5 inputs\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRCy_If86ty7",
        "outputId": "7b39d073-0ac0-4787-e23e-17c8fae24a0f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Shall I so much that poor infrunt, hair\n",
            "Shaltestors are at Clarence be it consent.\n",
            "\n",
            "RICHARD:\n",
            "Ay, with thy lips: set thy brawl sits, and cry hath he should so:\n",
            "But they shall speak from the cold conference?\n",
            "\n",
            "QUEEN MARGARET:\n",
            "So right have made a Jely, and thou, Heaven hath\n",
            "Done no free to do it for ports.\n",
            "\n",
            "First Senator:\n",
            "No, misturn than this.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Softly and to me? Both our hearts to find\n",
            "Where our state were too much in your nameless, and you\n",
            "Foul warrant him, a most disconfed enough;\n",
            "'moughts are did mine: bid them I might show her mind: conceive\n",
            "this oath I have any thing when stomands\n",
            "That still-hould bid give you on: upon that time\n",
            "It straits to hear a living him:\n",
            "And I have leave in trisun through the king did scont\n",
            "Of our devotion sworn like his opinion, where I give in\n",
            "joy; but your uncle York and Lewis\n",
            "besomity.\n",
            "\n",
            "JULIET:\n",
            "O, made not from the tomb;\n",
            "A sight remembered and left lowd:\n",
            "Enough for playing him,--do your love pussuned you.\n",
            "\n",
            "AUTOLYCUS:\n",
            "If they not, most \n",
            "________________________________________________________________________________\n",
            "ROMEO:\n",
            "O, sir, my lord, I'll to my thief arms. But, Clifford, bear my poaces!\n",
            "If any whet should she kneel down, is grown to stand found,\n",
            "Which are forget the shore, that had her not he is\n",
            "draw fast of a man of drop, these say the envious\n",
            "ceeth an officer, sir, but my attemper'd death.\n",
            "O possessed wears and suffering last,\n",
            "And blew their hearts, that more instructions\n",
            "and hand hillingly: give me some invireign,\n",
            "On pardon do good morsor, thou liest!\n",
            "\n",
            "JOHN OF GAUN:\n",
            "Then now deny to him with some most sorrow,\n",
            "And so become a brave Pelital: he\n",
            "made me four shall'd up: having the day come,\n",
            "Having warrabal and o'erwhelm come in proud\n",
            "At Angelo shall have thine humble\n",
            "That would usurp to him, and such a day--housand wicked\n",
            "Hability, that art so fond to wail\n",
            "The heart that to his most gracious mind\n",
            "Here takest the ears: and, to the rock, that it,\n",
            "And these intonest what let barness when thou art\n",
            "there?\n",
            "\n",
            "Shepherd:\n",
            "You are clemp with Mercutio, my haste goes\n",
            "To bet o' the offer;\n",
            "And, yet they know the \n",
            "________________________________________________________________________________\n",
            "ROMEO:\n",
            "\n",
            "VINCENTIO:\n",
            "That's a brother, let the infrom that\n",
            "Andight cushop off the visitate-robes.\n",
            "What is't a ground to solicit?\n",
            "For what o'clock, taste grace, father, effect win stmone.\n",
            "\n",
            "ROMEO:\n",
            "There is the store of prief; set down the\n",
            "southill of ourselves wrong your will. If you may,\n",
            "For what power for myself, I'll would confement\n",
            "I' the flatterers of her young prince as this happiness,\n",
            "And twenty times made meats it spresh of leath,\n",
            "The accusation and the wind of love;\n",
            "E'et goddens louder at Pomfret this,\n",
            "Or any of the book of me, whose meals, which, after\n",
            "What, his their life-up and look bat,\n",
            "The eastern of the commonweal, desires me with the sea\n",
            "shortla,--thy throat compaused, and cry 'D Clancaster,\n",
            "The generous and ignorant cannot lack\n",
            "Now secure and woman after to myself to-night\n",
            "Have thought the duke is so great and foul wrong.\n",
            "What, had the victor of our peevish father?\n",
            "\n",
            "BUCKINGHAM:\n",
            "MarG All Montague, and therefore I'll night,\n",
            "To her besure into a child; if please youth,\n",
            "Suff'd up th\n",
            "________________________________________________________________________________\n",
            "ROMEO:\n",
            "God you confing me to way.\n",
            "\n",
            "TYBALT:\n",
            "If these are adops; men's living king,\n",
            "Which, Tybalt, o' the earth, the shame soon-house to their\n",
            "festalousies, half a molemity of his master;\n",
            "And with the death of death the third if my thronething\n",
            "Wherein thyself, Margaret, and play mort\n",
            "To help with them, and put my ears to-morrow's thief:\n",
            "For thou shalt rise and leave of life, nor I nought\n",
            "With bleeding stain their frowns:\n",
            "Nor I'll not endule: I have it, France.\n",
            "\n",
            "WARWICK:\n",
            "True; but, like a mistress of his grey-riped\n",
            "From off my son.\n",
            "\n",
            "PETRUCHIO:\n",
            "Vilect, for the noble duke! Grumio, know, you were best has\n",
            "intended, elpender, so we may: but I can read'?\n",
            "And bid my sons wilt take and pity me;\n",
            "Farden Montgom of such tricipules fast,\n",
            "And bitterly you must says, overbears,\n",
            "And some incused itself and mercy how mine\n",
            "ere he were shaves jointed upon a coverture.\n",
            "\n",
            "LUCENTIO:\n",
            "I see: you would you do not to the petty.\n",
            "\n",
            "PETRUCHIO:\n",
            "Dost thou sound him! is this good deed, and there be hide,\n",
            "As you have possess'd\n",
            "________________________________________________________________________________\n",
            "ROMEO:\n",
            "The finer's vingerous trade, whe pute to our\n",
            "conscience in the cold.\n",
            "\n",
            "ROMEO:\n",
            "He bears thee not, this all, or malice offer\n",
            "The rarest thousand flay A golden cass\n",
            "that you must hall forward, daughter, and Romeo be\n",
            "The morninged and extremity be my contract.\n",
            "Then wherefore dost thou hold the people,\n",
            "And not revenge my death's saying's nature,\n",
            "I may be upon him.\n",
            "\n",
            "ISABELLA:\n",
            "O, he is very weak and womanions, I may show\n",
            "it did for sheer;\n",
            "And, sootefut thou a pray'r-people.\n",
            "\n",
            "SICINIUS:\n",
            "Where hath your husband--would thou mean the injury?\n",
            "Of this fast brother, have I thy heart i' the lives,\n",
            "Nor arm thee take the French king's crown,\n",
            "Have I left not breached and older\n",
            "Aboat'ster, when I am in a word your tears,\n",
            "And blown to thee. I had as lief.\n",
            "\n",
            "LADY ANNE:\n",
            "Adiguble and sovereign, after too much\n",
            "so himself unto whipping, and there peep and shoulders\n",
            "To worthy in my money from that gate of fir?\n",
            "\n",
            "ESCALUS:\n",
            "O my lady andome! the frantic to your father?\n",
            "\n",
            "BAPTISTA:\n",
            "I must be like you, we must back in h\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.3513824939727783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export the generator"
      ],
      "metadata": {
        "id": "0vLBcaFAAHEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olXS8zW59ye_",
        "outputId": "c26cf031-aca9-4500-fe43-579c3b69c6c5"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f9649cd49d0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}